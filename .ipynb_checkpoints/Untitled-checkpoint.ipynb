{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tech Tweet Sentiment Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello there! I have decided to conduct some sentiment analysis on tweets. The first part of this project was using Scala and Spark to stream the tweets into my MongoDB collection. Please take a look at the code for the first part. Let us begin! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Data From MongoDB  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve our data from the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import com.mongodb.spark._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming.{Seconds, StreamingContext}\n",
    "import org.apache.spark.streaming.twitter.TwitterUtils\n",
    "import org.bson.Document\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val conf = new SparkConf \n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.set(\"spark.executor.memory\",\"1g\")\n",
    "conf.setAppName(appName)\n",
    "conf.set(\"spark.mongodb.input.uri\",\"mongodb://127.0.0.1/twitter.net_tweets\")\n",
    "\n",
    "val sc = new SparkContext(conf, Seconds(10))\n",
    "\n",
    "val rdd = MongoSpark.load(sc)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala211",
   "name": "scala211"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
